{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "AllowedHosts": "*",
  "LLamaStackConfig": {
    "ModelLoadType": "Single",
    "ModelStatePath": "C:\\dev\\AI\\Models\\States",
    "Models": [
      {
        "Name": "WizardLM-7B",
        "MaxInstances": 2,
        "ModelPath": "C:\\dev\\AI\\Models\\llama-2-7b-guanaco-qlora.Q4_K_M.gguf",
        "ContextSize": 512,
        "BatchSize": 512,
        "Threads": 4,
        "GpuLayerCount": 20,
        "UseMemorymap": true,
        "UseMemoryLock": false,
        "MainGpu": 0,
        "LowVram": false,
        "Seed": 1686349486,
        "UseFp16Memory": true,
        "Perplexity": false,
        "ModelAlias": "unknown",
        "LoraAdapter": "",
        "LoraBase": "",
        "ConvertEosToNewLine": false,
        "EmbeddingMode": false,
        "TensorSplits": null,
        "GroupedQueryAttention": 1,
        "RmsNormEpsilon": 0.000005,
        "RopeFrequencyBase": 10000.0,
        "RopeFrequencyScale": 1.0,
        "MulMatQ": false,
        "Encoding": "UTF-8"
      },
      {
        "Name": "WizardLM-7B",
        "MaxInstances": 2,
        "ModelPath": "C:\\dev\\AI\\Models\\",
        "ContextSize": 512,
        "BatchSize": 512,
        "Threads": 4,
        "GpuLayerCount": 20,
        "UseMemorymap": true,
        "UseMemoryLock": false,
        "MainGpu": 0,
        "LowVram": false,
        "Seed": 1686349486,
        "UseFp16Memory": true,
        "Perplexity": false,
        "ModelAlias": "unknown",
        "LoraAdapter": "",
        "LoraBase": "",
        "ConvertEosToNewLine": false,
        "EmbeddingMode": false,
        "TensorSplits": null,
        "GroupedQueryAttention": 1,
        "RmsNormEpsilon": 0.000005,
        "RopeFrequencyBase": 10000.0,
        "RopeFrequencyScale": 1.0,
        "MulMatQ": false,
        "Encoding": "UTF-8"
      }
    ]
  }
}
